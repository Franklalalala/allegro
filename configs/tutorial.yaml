run: [train, test]

cutoff_radius: 5.0
chemical_symbols: [C, O, H]
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.sGDML_CCSD_DataModule
  dataset: aspirin
  data_source_dir: aspirin_data
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  trainval_test_subset: [40, 10]
  train_val_split: [30, 10]
  seed: 123
  train_dataloader_kwargs:
    batch_size: 1
  val_dataloader_kwargs:
    batch_size: 5
  test_dataloader_kwargs: ${data.val_dataloader_kwargs}
  stats_manager:
    _target_: nequip.data.DataStatisticsManager
    type_names: ${model_type_names}
    metrics:
      - field:
          _target_: nequip.data.NumNeighbors
        metric: 
          _target_: nequip.data.Mean
        name: num_neighbors_mean
      - field:
          _target_: nequip.data.PerAtomModifier
          field: total_energy
        metric:
          _target_: nequip.data.Mean
        name: per_atom_energy_mean
      - field: forces
        metric:
          _target_: nequip.data.RootMeanSquare
        per_type: true
        name: per_type_forces_rms

trainer:
  _target_: lightning.Trainer
  max_epochs: 5
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}
      save_last: true

training_module:
  _target_: nequip.train.NequIPLightningModule
  loss:
    _target_: nequip.train.MetricsManager
    metrics:
      - name: force_MSE
        field: forces
        coeff: 1
        metric:
          _target_: nequip.train.MeanSquaredError
  val_metrics: 
    _target_: nequip.train.MetricsManager
    metrics:
      - name: force_RMSE
        field: forces
        metric:
          _target_: nequip.train.RootMeanSquaredError
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam

  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # == two-body scalar embedding ==
    scalar_embed:
      # classic option is the Bessel scalar embedding module
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6
      two_body_embedding_dim: 32
      two_body_mlp_hidden_layer_depth: 2
      two_body_mlp_hidden_layer_width: 64
      two_body_mlp_nonlinearity: silu

      # one could also use the spline embedding module with less hyperparameters
      # _target_: allegro.nn.TwoBodySplineScalarEmbed
      # spline_grid: 5
      # spline_span: 3

    # one could explicitly specify the initial scalar embedding's output dimension
    # if `scalar_embed_output_dim` is not provided (or `null`), Allegro will use `num_scalar_features` in its place
    # scalar_embed_output_dim: 256

    # == symmetry ==
    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1
    # whether to include parity symmetry equivariance
    # allowed: o3_full, o3_restricted, so3
    parity_setting: o3_full   

    # == allegro layers ==
    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2
    
    # number of scalar features, more is more accurate but slower
    # powers of two (4, 8, 16, 64, 128, 256, 512, ...) are good options to try depending on data set
    num_scalar_features: 64
    
    # number of tensor features, more is more accurate but slower
    # powers of two (4, 8, 16, 64, 128, ...) are good options to try depending on data set
    num_tensor_features: 32

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is `false`, `true` may be useful for larger `l_max` and `num_layers` where there are more paths
    tp_path_channel_coupling: false

    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layer_depth: 2
    allegro_mlp_hidden_layer_width: 128
    allegro_mlp_nonlinearity: silu

    # == readout ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layer_depth: 1
    readout_mlp_hidden_layer_width: 32
    readout_mlp_nonlinearity: null
    # ^ setting `nonlinearity` to `null` means that output MLP is effectively a linear layer

    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # == per-type per-atom scales and shifts ==
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # == ZBL pair potential ==
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  seed: 789
  allow_tf32: false
